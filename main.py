# -*- coding: utf-8 -*-
"""
Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆå›½å†…8ç¤¾å¯¾å¿œç‰ˆãƒ»GitHub Actionsç”¨ï¼‰
 - Yahooã‚·ãƒ¼ãƒˆã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ï¼‹æœ¬æ–‡ï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰ã‚’æ›¸ãè¾¼ã¿
 - Commentsã‚·ãƒ¼ãƒˆã«ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰ã‚’æ›¸ãè¾¼ã¿
 - Gemini ã‚’ä½¿ã£ã¦ 100ä»¶ã¾ã¨ã‚ã¦ãƒãƒƒãƒåˆ†æã—ã€ä»¥ä¸‹ã‚’åˆ¤å®š
   * ä¸»é¡Œä¼æ¥­ (Påˆ—)
   * ã‚«ãƒ†ã‚´ãƒª (Qåˆ—)
   * ãƒã‚¸ãƒã‚¬ (Råˆ—)
   * æœ¬æ–‡ä¸­ã®æ—¥ç”£é–¢é€£æ–‡æŠ½å‡º (Såˆ—)
   * æœ¬æ–‡ä¸­ã®æ—¥ç”£ã«å¯¾ã™ã‚‹ãƒã‚¬ãƒ†ã‚£ãƒ–æ–‡æŠ½å‡º (Tåˆ—)
"""
import os
import json
import time
import re
import random
from datetime import datetime, timedelta, timezone
from typing import List, Optional, Dict, Any
import sys
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# --- Gemini API é–¢é€£ ---
from google import genai
from google.api_core.exceptions import ResourceExhausted

# ================== è¨­å®š ==================
SHARED_SPREADSHEET_ID = "1WUnLv7TIxY1-PPyLAmks2CEfAcfse51He32l79eUf7E"
KEYWORD_FILE = "keywords.txt"
SOURCE_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
SOURCE_SHEET_NAME = "Yahoo"
COMMENTS_SHEET_NAME = "Comments"
MAX_SHEET_ROWS_FOR_REPLACE = 10000

# æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
MAX_BODY_PAGES = 10
MAX_COMMENT_PAGES = 10

# ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ã®ä¸Šé™ï¼ˆãã‚Œä»¥ä¸Šã‚ã‚Œã°ã€Œâ€¦(over 3000)ã€ç­‰è¿½è¨˜ï¼‰
MAX_COMMENTS_TOTAL = 3000

# Yahoo ã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€å®šç¾©
# A:URL, B:ã‚¿ã‚¤ãƒˆãƒ«, C:æŠ•ç¨¿æ—¥æ™‚, D:ã‚½ãƒ¼ã‚¹,
# Eã€œN: æœ¬æ–‡(1ã€œ10ãƒšãƒ¼ã‚¸), O:ã‚³ãƒ¡ãƒ³ãƒˆæ•°,
# P:ä¸»é¡Œä¼æ¥­, Q:ã‚«ãƒ†ã‚´ãƒª, R:ãƒã‚¸ãƒã‚¬, S:æ—¥ç”£é–¢é€£æ–‡, T:æ—¥ç”£ãƒã‚¬æ–‡
YAHOO_SHEET_HEADERS = [
    "URL",  # A
    "ã‚¿ã‚¤ãƒˆãƒ«",  # B
    "æŠ•ç¨¿æ—¥æ™‚",  # C
    "ã‚½ãƒ¼ã‚¹",  # D
    "æœ¬æ–‡_P1",  # E
    "æœ¬æ–‡_P2",  # F
    "æœ¬æ–‡_P3",  # G
    "æœ¬æ–‡_P4",  # H
    "æœ¬æ–‡_P5",  # I
    "æœ¬æ–‡_P6",  # J
    "æœ¬æ–‡_P7",  # K
    "æœ¬æ–‡_P8",  # L
    "æœ¬æ–‡_P9",  # M
    "æœ¬æ–‡_P10", # N
    "ã‚³ãƒ¡ãƒ³ãƒˆæ•°",  # O
    "ä¸»é¡Œä¼æ¥­",    # P
    "ã‚«ãƒ†ã‚´ãƒª",    # Q
    "ãƒã‚¸ãƒã‚¬",    # R
    "æ—¥ç”£é–¢é€£æ–‡",  # S
    "æ—¥ç”£ãƒã‚¬æ–‡"   # T
]

# Comments ã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€å®šç¾©
# A:URL, B:ã‚¿ã‚¤ãƒˆãƒ«, C:æŠ•ç¨¿æ—¥æ™‚, D:ã‚½ãƒ¼ã‚¹, E:ã‚³ãƒ¡ãƒ³ãƒˆæ•°, Fã€œ:ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸
COMMENTS_SHEET_HEADERS = [
    "URL",    # A
    "ã‚¿ã‚¤ãƒˆãƒ«",  # B
    "æŠ•ç¨¿æ—¥æ™‚",  # C
    "ã‚½ãƒ¼ã‚¹",   # D
    "ã‚³ãƒ¡ãƒ³ãƒˆæ•°" # E
    # Fã€œ: ã‚³ãƒ¡ãƒ³ãƒˆ_P1ã€œP10 ã‚’è¿½åŠ 
]

REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}
TZ_JST = timezone(timedelta(hours=9))
PROMPT_FILES = [
    "prompt_gemini_role.txt",
    "prompt_posinega.txt",
    "prompt_category.txt",
    "prompt_target_company.txt"
]

# Gemini ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆ100ä»¶ã‚’1APIã§å‡¦ç†ï¼‰
GEMINI_MAX_BATCH_SIZE = 100

# ===== Gemini Client åˆæœŸåŒ– =====
try:
    GEMINI_CLIENT = genai.Client()
except Exception as e:
    print(f"è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    GEMINI_CLIENT = None

GEMINI_BATCH_PROMPT_BASE = None  # ãƒãƒƒãƒç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

# ================== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ==================
def to_str_safe(x) -> str:
    """None/æ•°å€¤/æ–‡å­—åˆ—ã‚’å®‰å…¨ã«æ–‡å­—åˆ—ã¸ã€‚å‰å¾Œç©ºç™½ã¯é™¤å»ã€‚"""
    if x is None:
        return ""
    try:
        return str(x).strip()
    except Exception:
        return ""

def jst_now() -> datetime:
    return datetime.now(TZ_JST)

def format_datetime(dt_obj: datetime) -> str:
    """yyyy/mm/dd hh:mm:ss å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    return dt_obj.strftime("%Y/%m/%d %H:%M:%S")

def parse_post_date(raw, today_jst: datetime) -> Optional[datetime]:
    """Yahooè¡¨ç¤ºã®æ—¥æ™‚æ–‡å­—åˆ—ã‚’ datetime(JST) ã«å¤‰æ›"""
    if raw is None:
        return None
    if isinstance(raw, (int, float)):
        return None
    if isinstance(raw, str):
        s = raw.strip()
        # ï¼ˆæœˆï¼‰ãªã©ã®æ›œæ—¥ã‚’å‰Šé™¤
        s = re.sub(r"\([\u670æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)", "", s).strip()
        # ã€Œé…ä¿¡ã€ã‚’å‰Šé™¤
        s = s.replace("é…ä¿¡", "").strip()
        for fmt in ("%Y/%m/%d %H:%M:%S", "%Y/%m/%d %H:%M", "%y/%m/%d %H:%M", "%m/%d %H:%M"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m/%d %H:%M":
                    dt = dt.replace(year=today_jst.year)
                if dt.replace(tzinfo=TZ_JST) > today_jst + timedelta(days=31):
                    dt = dt.replace(year=dt.year - 1)
                return dt.replace(tzinfo=TZ_JST)
            except ValueError:
                continue
    return None

def build_gspread_client() -> gspread.Client:
    """ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ gspread ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆ"""
    try:
        creds_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive"
        ]
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
            return gspread.authorize(credentials)
        else:
            return gspread.service_account(filename="credentials.json")
    except FileNotFoundError:
        raise RuntimeError("Googleèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ã¾ãŸã¯ credentials.json ã‚’ç¢ºèª)")
    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")

def load_keywords(filename: str) -> List[str]:
    """keywords.txt ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿"""
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        with open(file_path, "r", encoding="utf-8") as f:
            keywords = [line.strip() for line in f if line.strip() and not line.startswith("#")]
        if not keywords:
            raise ValueError("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã«æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return keywords
    except FileNotFoundError:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []
    except Exception as e:
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def load_gemini_batch_prompt() -> str:
    """
    PROMPT_FILES ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒƒãƒåˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆã€‚
    100ä»¶ã¾ã¨ã‚ã¦ JSON é…åˆ—ã§è¿”ã™ã‚ˆã†ã«æŒ‡ç¤ºã™ã‚‹ã€‚
    """
    global GEMINI_BATCH_PROMPT_BASE
    if GEMINI_BATCH_PROMPT_BASE is not None:
        return GEMINI_BATCH_PROMPT_BASE
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        role_path = os.path.join(script_dir, PROMPT_FILES[0])
        with open(role_path, "r", encoding="utf-8") as f:
            role_instruction = f.read().strip()

        other_contents = []
        for filename in PROMPT_FILES[1:]:
            path = os.path.join(script_dir, filename)
            with open(path, "r", encoding="utf-8") as f:
                c = f.read().strip()
                if c:
                    other_contents.append(c)

        if not role_instruction or not other_contents:
            print("è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒä¸å®Œå…¨ã¾ãŸã¯ç©ºã§ã™ã€‚")
            GEMINI_BATCH_PROMPT_BASE = ""
            return GEMINI_BATCH_PROMPT_BASE

        base = role_instruction + "\n" + "\n".join(other_contents)
        extra = """ 
è¿½åŠ è¦ä»¶:
- ä¸ãˆã‚‰ã‚Œã‚‹è¨˜äº‹æœ¬æ–‡ã¯æœ€å¤§ã§100ä»¶ã§ã™ã€‚
- ãã‚Œãã‚Œã®è¨˜äº‹ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
 * company_info: è¨˜äº‹ã®ä¸»é¡Œä¼æ¥­åã€‚å…±åŒé–‹ç™ºãªã©ãŒã‚ã‚Œã° () å†…ã«åˆ¥ä¼æ¥­ã‚‚æ›¸ã„ã¦ãã ã•ã„ã€‚
 * category: PROMPTã§æŒ‡å®šã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã«å¾“ã£ã¦ãã ã•ã„ã€‚
 * sentiment: è¨˜äº‹å…¨ä½“ã®ãƒˆãƒ¼ãƒ³ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ã®ã„ãšã‚Œã‹ã§åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
 * nissan_related: è¨˜äº‹æœ¬æ–‡ä¸­ã§ã€Œæ—¥ç”£ã€ã‚„ã€ŒNISSANã€ã€Œãƒ‹ãƒƒã‚µãƒ³ã€ãªã©ã®è¨€åŠæ–‡ã‚’æŠ½å‡ºã€‚ãªã‘ã‚Œã° "N/A"ã€‚
 * nissan_negative: ä¸Šè¨˜ã®ã†ã¡ãƒã‚¬ãƒ†ã‚£ãƒ–ãªå°è±¡ã®æ–‡ã®ã¿æŠ½å‡ºã€‚ãªã‘ã‚Œã° "N/A"ã€‚

å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: JSON é…åˆ—ï¼ˆå„è¦ç´ : index, company_info, category, sentiment, nissan_related, nissan_negativeï¼‰
å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: "==== ARTICLE i START ===="ã€œ"==== ARTICLE i END ====" ãŒ index=i ã®æœ¬æ–‡
"""
        base += "\n\n" + extra + "\n\n{TEXT_BATCH}"
        GEMINI_BATCH_PROMPT_BASE = base
        print(f"Gemini ãƒãƒƒãƒç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ {PROMPT_FILES} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return GEMINI_BATCH_PROMPT_BASE
    except FileNotFoundError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å: {e.filename}")
        GEMINI_BATCH_PROMPT_BASE = ""
        return GEMINI_BATCH_PROMPT_BASE
    except Exception as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        GEMINI_BATCH_PROMPT_BASE = ""
        return GEMINI_BATCH_PROMPT_BASE

def request_with_retry(url: str, max_retries: int = 3) -> Optional[requests.Response]:
    """requests.get ã‚’ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§å®Ÿè¡Œã€‚404 ã®å ´åˆã¯å³ None"""
    for attempt in range(max_retries):
        try:
            res = requests.get(url, headers=REQ_HEADERS, timeout=20)
            if res.status_code == 404:
                print(f" âŒ 404 Not Found: {url}")
                return None
            res.raise_for_status()
            return res
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt + random.random()
                print(f" âš ï¸ æ¥ç¶šã‚¨ãƒ©ãƒ¼, ãƒªãƒˆãƒ©ã‚¤ {attempt+1}/{max_retries} ({wait_time:.2f}ç§’): {e}")
                time.sleep(wait_time)
            else:
                print(f" âŒ æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤å¤±æ•—: {e}")
                return None
    return None

# ================== æ¤œç´¢çµæœã‹ã‚‰è¨˜äº‹URLæŠ½å‡ºï¼ˆHTMLï¼‹ãƒ†ã‚­ã‚¹ãƒˆä¸¡å¯¾å¿œï¼‰ ==================
def extract_article_urls(page_html: str) -> List[str]:
    """
    æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã® HTML/ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€è¨˜äº‹URLï¼ˆhttps://news.yahoo.co.jp/articles/xxxxxï¼‰ã‚’åé›†ã€‚
    BeautifulSoupã§ã®<a>æŠ½å‡ºã¨ã€æ­£è¦è¡¨ç¾ã®ä¸¡æ–¹ã§æ‹¾ã„ã€é‡è¤‡é™¤å»ã—ã¦è¿”ã™ã€‚
    """
    urls: List[str] = []

    # 1) HTMLã‚¢ãƒ³ã‚«ãƒ¼ã‹ã‚‰æŠ½å‡º
    try:
        soup = BeautifulSoup(page_html, "html.parser")
        for a in soup.select('a[href^="https://news.yahoo.co.jp/articles/"]'):
            href = a.get("href", "")
            if href and href.startswith("https://news.yahoo.co.jp/articles/"):
                urls.append(href)
    except Exception:
        pass

    # 2) ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸­ã® URL ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡ºï¼ˆMarkdowné¢¨ã®ãƒ†ã‚­ã‚¹ãƒˆã«ã‚‚å¯¾å¿œï¼‰
    try:
        regex = re.compile(r"https://news\.yahoo\.co\.jp/articles/[A-Za-z0-9]+")
        urls.extend(regex.findall(page_html))
    except Exception:
        pass

    # é‡è¤‡é™¤å»ï¼†é †åºä¿æŒ
    seen = set()
    unique_urls = []
    for u in urls:
        if u not in seen:
            seen.add(u)
            unique_urls.append(u)

    return unique_urls

# ================== è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ¡ã‚¿æƒ…å ±å–å¾— ==================
def fetch_article_meta(url: str) -> Dict[str, str]:
    """
    è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰ ã‚¿ã‚¤ãƒˆãƒ«ï¼æŠ•ç¨¿æ—¥æ™‚ï¼ˆæ–‡å­—åˆ—ï¼‰ï¼ã‚½ãƒ¼ã‚¹ï¼ˆæä¾›ç¤¾åï¼‰ ã‚’å–å¾—ã€‚
    ãƒšãƒ¼ã‚¸æ§‹é€ ã®å·®ç•°ã«è€ãˆã‚‹ãŸã‚ã€è¤‡æ•°ã®å€™è£œã‚’è©¦ã™ã€‚
    """
    meta = {"ã‚¿ã‚¤ãƒˆãƒ«": "", "æŠ•ç¨¿æ—¥æ™‚": "", "ã‚½ãƒ¼ã‚¹": ""}

    res = request_with_retry(url)
    if not res:
        return meta

    soup = BeautifulSoup(res.text, "html.parser")

    # ã‚¿ã‚¤ãƒˆãƒ«: h1 ã¾ãŸã¯ og:title
    title = ""
    h1 = soup.find("h1")
    if h1 and to_str_safe(h1.text):
        title = to_str_safe(h1.text)
    else:
        og = soup.find("meta", attrs={"property": "og:title"})
        if og and og.get("content"):
            title = to_str_safe(og.get("content"))

    # æ—¥æ™‚: <time> ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å„ªå…ˆ
    post_str = ""
    time_tag = soup.find("time")
    if time_tag and to_str_safe(time_tag.text):
        post_str = to_str_safe(time_tag.text)
    else:
        # æ–‡ç« å†…ã‹ã‚‰ "11/19(æ°´)17:45é…ä¿¡" ãªã©ã‚’æ¢ç´¢
        m = re.search(r"\d{1,2}/\d{1,2}\(.+?\)\d{1,2}:\d{2}\s*é…ä¿¡", soup.get_text())
        if m:
            post_str = to_str_safe(m.group(0))

    # ã‚½ãƒ¼ã‚¹: æä¾›ç¤¾ãƒªãƒ³ã‚¯ã‚„ãƒ©ãƒ™ãƒ«
    source = ""
    # æä¾›ç¤¾ã¸ã®ãƒªãƒ³ã‚¯ã£ã½ã„è¦ç´ 
    provider_candidates = [
        ("a", {"href": re.compile(r"/media/")}),
        ("a", {"class": re.compile(r"provider|media", re.I)}),
        ("span", {"class": re.compile(r"provider|media", re.I)}),
        ("div", {"class": re.compile(r"provider|media", re.I)}),
    ]
    for name, attrs in provider_candidates:
        el = soup.find(name, attrs=attrs)
        if el and to_str_safe(el.get_text()):
            source = to_str_safe(el.get_text())
            break
    if not source:
        # ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å€™è£œæŠ½å‡ºï¼ˆä¾‹: "æ™‚äº‹é€šä¿¡", "æ—¥åˆŠã‚¹ãƒãƒ¼ãƒ„" ãªã©ï¼‰
        candidates = ["æ™‚äº‹é€šä¿¡", "æ—¥åˆŠã‚¹ãƒãƒ¼ãƒ„", "ã‚¹ãƒãƒ¼ãƒ„å ±çŸ¥", "J-CASTãƒ‹ãƒ¥ãƒ¼ã‚¹", "æ²–ç¸„ã‚¿ã‚¤ãƒ ã‚¹",
                      "å…±åŒé€šä¿¡", "æœæ—¥æ–°è", "èª­å£²æ–°è", "æ¯æ—¥æ–°è", "ç”£çµŒæ–°è", "NHK"]
        text = soup.get_text()
        for name in candidates:
            if name in text:
                source = name
                break

    meta["ã‚¿ã‚¤ãƒˆãƒ«"] = title
    meta["æŠ•ç¨¿æ—¥æ™‚"] = post_str if post_str else "å–å¾—ä¸å¯"
    meta["ã‚½ãƒ¼ã‚¹"]   = source if source else "å–å¾—ä¸å¯"
    return meta

# ================== Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ (Selenium) ==================
def get_yahoo_news_with_selenium(keyword: str) -> List[Dict[str, str]]:
    print(f" Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword})...")
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={REQ_HEADERS['User-Agent']}")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)

    try:
        driver = webdriver.Chrome(options=options)
    except Exception as e:
        print(f" WebDriver åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

    search_url = (
        f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
        "&categories=domestic,world,business,it,science,life,local"
    )
    driver.get(search_url)
    time.sleep(3)  # æ¤œç´¢çµæœã®åˆæœŸãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾…ã¡ï¼ˆæ§‹é€ å¤‰åŒ–å¯¾ç­–ã§å›ºå®šç§’ï¼‰

    page_html = driver.page_source
    driver.quit()

    # è¨˜äº‹URLã®æŠ½å‡ºï¼ˆHTMLï¼‹ãƒ†ã‚­ã‚¹ãƒˆä¸¡å¯¾å¿œï¼‰
    urls = extract_article_urls(page_html)
    if not urls:
        print(" âš ï¸ è¨˜äº‹URLã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ§‹é€ å¤‰æ›´ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return []

    articles_data: List[Dict[str, str]] = []
    for url in urls:
        # è¨˜äº‹ãƒšãƒ¼ã‚¸å´ã‹ã‚‰ãƒ¡ã‚¿æƒ…å ±å–å¾—ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»æ—¥æ™‚ãƒ»ã‚½ãƒ¼ã‚¹ï¼‰
        meta = fetch_article_meta(url)
        if url:
            articles_data.append({
                "URL": url,
                "ã‚¿ã‚¤ãƒˆãƒ«": meta.get("ã‚¿ã‚¤ãƒˆãƒ«", "") or "",
                "æŠ•ç¨¿æ—¥æ™‚": meta.get("æŠ•ç¨¿æ—¥æ™‚", "") or "å–å¾—ä¸å¯",
                "ã‚½ãƒ¼ã‚¹": meta.get("ã‚½ãƒ¼ã‚¹", "") or "å–å¾—ä¸å¯",
            })

    print(f" Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(articles_data)} ä»¶å–å¾—")
    return articles_data

# ================== ã‚·ãƒ¼ãƒˆæ“ä½œãƒ˜ãƒ«ãƒ‘ãƒ¼ ==================
def ensure_yahoo_sheet(gc: gspread.Client) -> gspread.Worksheet:
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(
            title=SOURCE_SHEET_NAME,
            rows=str(MAX_SHEET_ROWS_FOR_REPLACE),
            cols=str(len(YAHOO_SHEET_HEADERS)),
        )
    headers = ws.row_values(1)
    if headers != YAHOO_SHEET_HEADERS:
        ws.update(
            range_name=f"A1:{gspread.utils.rowcol_to_a1(1, len(YAHOO_SHEET_HEADERS))}",
            values=[YAHOO_SHEET_HEADERS],
        )
    return ws

def ensure_comments_sheet(gc: gspread.Client) -> gspread.Worksheet:
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(COMMENTS_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(
            title=COMMENTS_SHEET_NAME,
            rows=str(MAX_SHEET_ROWS_FOR_REPLACE),
            cols=str(5 + MAX_COMMENT_PAGES),
        )
        ws.update(
            range_name=f"A1:{gspread.utils.rowcol_to_a1(1, 5 + MAX_COMMENT_PAGES)}",
            values=[COMMENTS_SHEET_HEADERS + [f"ã‚³ãƒ¡ãƒ³ãƒˆ_P{i}" for i in range(1, MAX_COMMENT_PAGES + 1)]],
        )
    return ws

def write_news_list_to_source(gc: gspread.Client, articles: List[Dict[str, str]]) -> None:
    ws = ensure_yahoo_sheet(gc)
    existing = ws.get_all_values(value_render_option="UNFORMATTED_VALUE")
    existing_urls = set(
        to_str_safe(row[0])
        for row in existing[1:]
        if len(row) > 0 and to_str_safe(row[0]).startswith("http")
    )

    new_rows = []
    for a in articles:
        if a["URL"] in existing_urls:
            continue
        row = [
            a["URL"],
            a["ã‚¿ã‚¤ãƒˆãƒ«"],
            a["æŠ•ç¨¿æ—¥æ™‚"],
            a["ã‚½ãƒ¼ã‚¹"],
        ]
        row.extend([""] * (len(YAHOO_SHEET_HEADERS) - len(row)))
        new_rows.append(row)

    if new_rows:
        ws.append_rows(new_rows, value_input_option="USER_ENTERED")
        print(f" Yahooã‚·ãƒ¼ãƒˆã« {len(new_rows)} ä»¶è¿½è¨˜ã—ã¾ã—ãŸã€‚")
    else:
        print(" è¿½è¨˜ã™ã¹ãæ–°è¦è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

# ================== æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— ==================
def fetch_article_body_page(url: str) -> str:
    """è¨˜äº‹ã®å˜ä¸€ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    res = request_with_retry(url)
    if not res:
        return ""
    soup = BeautifulSoup(res.text, "html.parser")
    article_content = (
        soup.find("article")
        or soup.find("div", class_="article_body")
        or soup.find("div", class_=re.compile(r"article_detail|article_body"))
    )
    texts: List[str] = []
    if article_content:
        paragraphs = article_content.find_all("p", class_=re.compile(r"highLightSearchTarget"))
        if not paragraphs:
            paragraphs = article_content.find_all("p")
        for p in paragraphs:
            t = p.get_text(strip=True)
            if t:
                texts.append(t)
    return "\n".join(texts).strip()

def fetch_comments_page(url: str) -> List[str]:
    """
    ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸1æšåˆ†ã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã‚’ãƒªã‚¹ãƒˆã§è¿”ã™ã€‚
    Yahooã®HTMLæ§‹é€ å¤‰åŒ–ã«å¼·ã‚ã®â€œç¯„å›²æŒ‡å®šï¼‹é™å®šã‚»ãƒ¬ã‚¯ã‚¿â€ï¼‹â€œãƒã‚¤ã‚ºé™¤å»â€ã§æŠ½å‡ºã™ã‚‹ã€‚
    """
    res = request_with_retry(url)
    if not res:
        return []

    soup = BeautifulSoup(res.text, "html.parser")

    # ã‚³ãƒ¡ãƒ³ãƒˆé ˜åŸŸã®èµ·ç‚¹å€™è£œ
    container_candidates = [
        {"name": "section", "attrs": {"id": re.compile(r"^comments?$", re.I)}},
        {"name": "section", "attrs": {"data-testid": re.compile(r"comment", re.I)}},
        {"name": "div",     "attrs": {"id": re.compile(r"^comment", re.I)}},
        {"name": "div",     "attrs": {"class": re.compile(r"^comment", re.I)}},
        {"name": "div",     "attrs": {"class": re.compile(r"CommentItem")}},
        {"name": "section", "attrs": {"class": re.compile(r"comments|Comment", re.I)}},
    ]

    root = None
    for cand in container_candidates:
        root = soup.find(cand["name"], attrs=cand["attrs"])
        if root:
            break
    scope = root if root else soup

    comments: List[str] = []

    # ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã£ã½ã„é™å®šã‚»ãƒ¬ã‚¯ã‚¿ã®ã¿
    strict_selectors = [
        "div[class*='CommentItem__body']",
        "div[class*='CommentItem__text']",
        "p[class*='CommentItem__body']",
        "p[class*='CommentItem__text']",
        "span[class*='CommentItem__body']",
        "span[class*='CommentItem__text']",
        "div[class*='CommentItem'] p",
        "div[class*='CommentItem'] span",
    ]

    # ãƒã‚¤ã‚ºå®šå‹æ–‡é™¤å»
    noise_patterns = [
        r"^\s*ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ›¸ã\s*$",
        r"^\s*ãƒ¤ãƒ•ã‚³ãƒ¡ãƒãƒªã‚·ãƒ¼\s*$",
        r"^\s*PayPayæ®‹é«˜ä½¿ãˆã¾ã™\s*$",
        r"ç¨è¾¼\s*\d+\s*å††",
        r"\d{1,2}/\d{1,2}\(.+?\)\d{1,2}:\d{2}\s*é…ä¿¡",
        r"^\s*ABEMA\s*TIMES.*é…ä¿¡\s*$",
        r"^\s*æ²–ç¸„ã‚¿ã‚¤ãƒ ã‚¹.*$",
        r"^\s*ã€.+?ã€‘.*$",
        r"^\s*å‰è·.*é¸.*$",
    ]
    noise_re = re.compile("|".join(noise_patterns), re.I)

    def is_comment_text(text: str) -> bool:
        if not text:
            return False
        if len(text) < 6:
            return False
        if noise_re.search(text):
            return False
        return True

    for sel in strict_selectors:
        for node in scope.select(sel):
            text = node.get_text(strip=True)
            if is_comment_text(text) and text not in comments:
                comments.append(text)

    # ä½•ã‚‚æ‹¾ãˆãªã„å ´åˆã®æœ€çµ‚æ‰‹æ®µï¼ˆç¯„å›²é™å®šï¼‰
    if not comments and root:
        for node in root.find_all(["p", "span", "div"]):
            cls = " ".join(node.get("class", []))
            if not re.search(r"CommentItem|comment", cls, re.I):
                continue
            text = node.get_text(strip=True)
            if is_comment_text(text) and text not in comments:
                comments.append(text)

    return comments

def fetch_details_and_update(gc: gspread.Client) -> None:
    """
    å„è¡Œã«ã¤ã„ã¦:
     - æœ¬æ–‡10ãƒšãƒ¼ã‚¸åˆ† (Eã€œN) ã‚’å–å¾—ãƒ»æ›´æ–°
     - ã‚³ãƒ¡ãƒ³ãƒˆæ•° (O) ã‚’æ›´æ–°
     - Commentsã‚·ãƒ¼ãƒˆã«ã‚³ãƒ¡ãƒ³ãƒˆ10ãƒšãƒ¼ã‚¸åˆ†ã‚’æ›¸ãè¾¼ã¿
    æ—¢ã«æœ¬æ–‡P1ãŒå…¥ã£ã¦ã„ã‚‹è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå†å–å¾—ã—ãªã„ï¼‰
    """
    yahoo_ws = ensure_yahoo_sheet(gc)
    comments_ws = ensure_comments_sheet(gc)
    values = yahoo_ws.get_all_values(value_render_option="UNFORMATTED_VALUE")
    if len(values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    data_rows = values[1:]
    print("\n===== ğŸ“„ ã‚¹ãƒ†ãƒƒãƒ—2: æœ¬æ–‡ï¼†ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ãƒ»Commentsã‚·ãƒ¼ãƒˆæ›´æ–° =====")

    comments_values = comments_ws.get_all_values(value_render_option="UNFORMATTED_VALUE")
    comments_url_to_row: Dict[str, int] = {}
    if comments_values:
        for idx, row in enumerate(comments_values[1:], start=2):
            if len(row) > 0 and to_str_safe(row[0]).startswith("http"):
                comments_url_to_row[to_str_safe(row[0])] = idx

    update_body_count = 0
    update_comments_count = 0

    for idx, row in enumerate(data_rows, start=2):
        if len(row) < len(YAHOO_SHEET_HEADERS):
            row.extend([""] * (len(YAHOO_SHEET_HEADERS) - len(row)))

        url = to_str_safe(row[0])
        if not url.startswith("https://news.yahoo.co.jp/articles/"):
            continue

        title = to_str_safe(row[1])
        post_date = to_str_safe(row[2])
        source = to_str_safe(row[3])

        body_pages = row[4:4 + MAX_BODY_PAGES]
        comment_count_str = to_str_safe(row[14]) if len(row) > 14 else ""

        need_body = not to_str_safe(body_pages[0])
        need_comments = True

        new_body_pages = list(body_pages)
        new_comment_count = comment_count_str

        if need_body or need_comments:
            print(f" - è¡Œ {idx} (è¨˜äº‹: {title[:20]}...): è©³ç´°å–å¾—ä¸­...")

        # æœ¬æ–‡å–å¾—ï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰
        if need_body:
            body_changed = False
            for page_idx in range(1, MAX_BODY_PAGES + 1):
                page_url = url if page_idx == 1 else f"{url}?page={page_idx}"
                text = fetch_article_body_page(page_url)
                if not text:
                    if page_idx == 1:
                        new_body_pages[0] = "æœ¬æ–‡å–å¾—ä¸å¯"
                        body_changed = True
                    break

                col_idx = page_idx - 1
                if to_str_safe(new_body_pages[col_idx]) != text:
                    new_body_pages[col_idx] = text
                    body_changed = True

            if body_changed:
                yahoo_ws.update(
                    range_name=f"E{idx}:N{idx}",
                    values=[new_body_pages],
                    value_input_option="USER_ENTERED",
                )
                update_body_count += 1
                time.sleep(1 + random.random() * 0.5)

        # ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰
        if need_comments:
            all_comments: List[str] = []
            page_strings: List[str] = [""] * MAX_COMMENT_PAGES

            for page_idx in range(1, MAX_COMMENT_PAGES + 1):
                c_url = url + ("/comments" if page_idx == 1 else f"/comments?page={page_idx}")
                comments = fetch_comments_page(c_url)
                if not comments:
                    break

                if len(all_comments) + len(comments) > MAX_COMMENTS_TOTAL:
                    remaining = MAX_COMMENTS_TOTAL - len(all_comments)
                    if remaining > 0:
                        comments = comments[:remaining]
                        all_comments.extend(comments)
                    numbered = [f"[{i}] {c}" for i, c in enumerate(comments, start=1)]
                    page_strings[page_idx - 1] = "\n\n".join(numbered) + "\n\n(* over 3000 comments, truncated)"
                    break
                else:
                    all_comments.extend(comments)
                    numbered = [f"[{i}] {c}" for i, c in enumerate(comments, start=1)]
                    page_strings[page_idx - 1] = "\n\n".join(numbered)

            new_comment_count = str(len(all_comments))

            yahoo_ws.update(
                range_name=f"O{idx}:O{idx}",
                values=[[new_comment_count]],
                value_input_option="USER_ENTERED",
            )
            update_comments_count += 1

            base_vals = [url, title, post_date, source, new_comment_count]
            base_vals.extend(page_strings)

            if url in comments_url_to_row:
                c_row = comments_url_to_row[url]
                comments_ws.update(
                    range_name=f"A{c_row}:{gspread.utils.rowcol_to_a1(c_row, 5 + MAX_COMMENT_PAGES)}",
                    values=[base_vals],
                    value_input_option="USER_ENTERED",
                )
            else:
                comments_ws.append_row(base_vals, value_input_option="USER_ENTERED")
                new_row_index = len(comments_values) + 1 + len(comments_url_to_row)
                comments_url_to_row[url] = new_row_index

            time.sleep(1 + random.random() * 0.5)

    print(f" âœ… æœ¬æ–‡å–å¾—ã‚’ {update_body_count} è¡Œã«å®Ÿè¡Œ")
    print(f" âœ… ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ï¼†Commentsã‚·ãƒ¼ãƒˆæ›´æ–°ã‚’ {update_comments_count} è¡Œã«å®Ÿè¡Œ")

# ================== Gemini ãƒãƒƒãƒåˆ†æ ==================
def analyze_with_gemini_batch(texts: List[str]) -> List[Dict[str, str]]:
    """æœ€å¤§100ä»¶ã®æœ¬æ–‡ã‚’ã¾ã¨ã‚ã¦ Gemini ã§åˆ†æã—ã€JSONé…åˆ—ã‚’è¿”ã™ã€‚"""
    if not GEMINI_CLIENT or not texts:
        return []

    if len(texts) > GEMINI_MAX_BATCH_SIZE:
        texts = texts[:GEMINI_MAX_BATCH_SIZE]

    prompt_template = load_gemini_batch_prompt()
    if not prompt_template:
        print("Geminiãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºã®ãŸã‚ã€ãƒãƒƒãƒåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []

    trimmed_texts = [t[:3000] for t in texts]
    blocks = [
        f"==== ARTICLE {i} START ====\n{txt}\n==== ARTICLE {i} END ===="
        for i, txt in enumerate(trimmed_texts)
    ]
    text_batch = "\n\n".join(blocks)
    prompt = prompt_template.replace("{TEXT_BATCH}", text_batch)

    MAX_RETRIES = 3
    for attempt in range(MAX_RETRIES):
        try:
            resp = GEMINI_CLIENT.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config={"response_mime_type": "application/json"},
            )
            raw = to_str_safe(resp.text)
            try:
                data = json.loads(raw)
            except json.JSONDecodeError:
                print("Gemini ã‹ã‚‰ã® JSON ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è¿”å´ãƒ†ã‚­ã‚¹ãƒˆ:")
                print(raw)
                return []

            if isinstance(data, dict) and "results" in data:
                results = data["results"]
            elif isinstance(data, list):
                results = data
            else:
                print("Geminiå¿œç­”JSONå½¢å¼ãŒæƒ³å®šå¤–: dict(results=...) ã¾ãŸã¯ list[...] ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                return []

            out: List[Dict[str, str]] = []
            for item in results:
                if not isinstance(item, dict):
                    continue
                out.append(
                    {
                        "index": item.get("index", 0),
                        "company_info": item.get("company_info", "N/A"),
                        "category": item.get("category", "N/A"),
                        "sentiment": item.get("sentiment", "N/A"),
                        "nissan_related": item.get("nissan_related", "N/A"),
                        "nissan_negative": item.get("nissan_negative", "N/A"),
                    }
                )
            return out

        except ResourceExhausted as e:
            print(f" ğŸš¨ Gemini API ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
            raise
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                wait = 2 ** attempt + random.random()
                print(f" âš ï¸ Gemini API ä¸€æ™‚ã‚¨ãƒ©ãƒ¼ã€‚{wait:.2f}ç§’å¾Œã«å†è©¦è¡Œ ({attempt+1}/{MAX_RETRIES})\n {e}")
                time.sleep(wait)
            else:
                print(f"Geminiãƒãƒƒãƒåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                return []
    return []

def analyze_and_update_sheet(gc: gspread.Client) -> None:
    """æœ¬æ–‡ã®å…¥ã£ã¦ã„ã‚‹è¡Œã® Pã€œT åˆ—ã‚’ Gemini åˆ†æã§æ›´æ–°"""
    if not GEMINI_CLIENT:
        print("Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    ws = ensure_yahoo_sheet(gc)
    values = ws.get_all_values(value_render_option="UNFORMATTED_VALUE")
    if len(values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€Geminiåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    data_rows = values[1:]
    print("\n===== ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—3: Geminiãƒãƒƒãƒåˆ†æ (100ä»¶/1API) =====")

    targets: List[Dict[str, Any]] = []
    for idx, row in enumerate(data_rows, start=2):
        if len(row) < len(YAHOO_SHEET_HEADERS):
            row.extend([""] * (len(YAHOO_SHEET_HEADERS) - len(row)))

        url = to_str_safe(row[0])
        if not url.startswith("https://news.yahoo.co.jp/articles/"):
            continue

        pages = row[4:4 + MAX_BODY_PAGES]
        pages = [p for p in pages if to_str_safe(p) and p != "æœ¬æ–‡å–å¾—ä¸å¯"]
        if not pages:
            continue

        body_text = "\n\n".join(
            f"ã€Page{i+1}ã€‘\n{to_str_safe(p)}" for i, p in enumerate(pages)
        )

        company_info = to_str_safe(row[15]) if len(row) > 15 else ""
        category     = to_str_safe(row[16]) if len(row) > 16 else ""
        sentiment    = to_str_safe(row[17]) if len(row) > 17 else ""
        nissan_rel   = to_str_safe(row[18]) if len(row) > 18 else ""
        nissan_neg   = to_str_safe(row[19]) if len(row) > 19 else ""
        if company_info and category and sentiment and nissan_rel and nissan_neg:
            continue

        targets.append(
            {
                "row_index": idx,
                "url": url,
                "title": to_str_safe(row[1]),
                "body": body_text,
            }
        )

    if not targets:
        print(" Geminiåˆ†æãŒå¿…è¦ãªè¡Œã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f" Geminiåˆ†æå¯¾è±¡: {len(targets)} è¡Œ")
    updated_count = 0

    for i in range(0, len(targets), GEMINI_MAX_BATCH_SIZE):
        batch = targets[i : i + GEMINI_MAX_BATCH_SIZE]
        texts = [t["body"] for t in batch]
        print(f" - {i+1}ã€œ{i+len(batch)}ä»¶ç›®ã‚’ãƒãƒƒãƒåˆ†æä¸­...")

        try:
            results = analyze_with_gemini_batch(texts)
        except ResourceExhausted:
            print(" ğŸš¨ Geminiã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«åˆ°é”ã—ãŸãŸã‚ã€æ®‹ã‚Šã®åˆ†æã¯æ¬¡å›ã¸æŒã¡è¶Šã—ã¾ã™ã€‚")
            break

        result_by_index: Dict[int, Dict[str, str]] = {}
        for r in results:
            try:
                idx_int = int(r.get("index", 0))
            except Exception:
                idx_int = 0
            result_by_index[idx_int] = r

        for local_idx, item in enumerate(batch):
            row_idx = item["row_index"]
            r = result_by_index.get(local_idx)
            if not r:
                continue

            company_info = r.get("company_info", "N/A")
            category     = r.get("category", "N/A")
            sentiment    = r.get("sentiment", "N/A")
            nissan_rel   = r.get("nissan_related", "N/A")
            nissan_neg   = r.get("nissan_negative", "N/A")

            ws.update(
                range_name=f"P{row_idx}:T{row_idx}",
                values=[[company_info, category, sentiment, nissan_rel, nissan_neg]],
                value_input_option="USER_ENTERED",
            )
            updated_count += 1
            time.sleep(0.8 + random.random() * 0.4)

    print(f" âœ… Geminiãƒãƒƒãƒåˆ†æçµæœã‚’ {updated_count} è¡Œã«åæ˜ ã—ã¾ã—ãŸã€‚")

# ================== ãƒ¡ã‚¤ãƒ³å‡¦ç† ==================
def main():
    print("--- Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼é–‹å§‹ ---")
    keywords = load_keywords(KEYWORD_FILE)
    if not keywords:
        print("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„ãŸã‚çµ‚äº†ã—ã¾ã™ã€‚")
        sys.exit(0)

    try:
        gc = build_gspread_client()
    except RuntimeError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã—ã¦ Yahoo ã‚·ãƒ¼ãƒˆã«è¿½è¨˜
    for kw in keywords:
        print(f"\n===== ğŸ”‘ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—: {kw} =====")
        articles = get_yahoo_news_with_selenium(kw)
        write_news_list_to_source(gc, articles)
        time.sleep(2)

    # ã‚¹ãƒ†ãƒƒãƒ—2: æœ¬æ–‡ï¼†ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— + Commentsã‚·ãƒ¼ãƒˆæ›´æ–°
    fetch_details_and_update(gc)

    # ã‚¹ãƒ†ãƒƒãƒ—3: Geminiãƒãƒƒãƒåˆ†æ (100ä»¶/1API)
    analyze_and_update_sheet(gc)

    print("\n--- Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Œäº† ---")

if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))
    if script_dir not in sys.path:
        sys.path.append(script_dir)
    main()
