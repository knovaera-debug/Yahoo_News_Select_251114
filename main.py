# -*- coding: utf-8 -*-
"""
Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆå›½å†…8ç¤¾å¯¾å¿œç‰ˆãƒ»GitHub Actionsç”¨ï¼‰
 - Yahooã‚·ãƒ¼ãƒˆã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ï¼‹æœ¬æ–‡ï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰ã‚’æ›¸ãè¾¼ã¿
 - Commentsã‚·ãƒ¼ãƒˆã«ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰ã‚’æ›¸ãè¾¼ã¿
 - Gemini ã‚’ä½¿ã£ã¦ 10ä»¶ã¾ã¨ã‚ã¦ãƒãƒƒãƒåˆ†æã—ã€ä»¥ä¸‹ã‚’åˆ¤å®š
    * ä¸»é¡Œä¼æ¥­ (Påˆ—)
    * ã‚«ãƒ†ã‚´ãƒª (Qåˆ—)
    * ãƒã‚¸ãƒã‚¬ (Råˆ—)
    * æœ¬æ–‡ä¸­ã®æ—¥ç”£é–¢é€£æ–‡æŠ½å‡º (Såˆ—)
    * æœ¬æ–‡ä¸­ã®æ—¥ç”£ã«å¯¾ã™ã‚‹ãƒã‚¬ãƒ†ã‚£ãƒ–æ–‡æŠ½å‡º (Tåˆ—)
"""

import os
import json
import time
import re
import random
from datetime import datetime, timedelta, timezone
from typing import List, Tuple, Optional, Dict, Any
import sys

from urllib.parse import urlparse, parse_qs, urlunparse, urlencode

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup
import requests

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- Gemini API é–¢é€£ ---
from google import genai
from google.api_core.exceptions import ResourceExhausted

# ================== è¨­å®š ==================

SHARED_SPREADSHEET_ID = "1WUnLv7TIxY1-PPyLAmks2CEfAcfse51He32l79eUf7E"
KEYWORD_FILE = "keywords.txt"

SOURCE_SPREADSHEET_ID = SHARED_SPREADSHEET_ID
SOURCE_SHEET_NAME = "Yahoo"
COMMENTS_SHEET_NAME = "Comments"

MAX_SHEET_ROWS_FOR_REPLACE = 10000

# æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
MAX_BODY_PAGES = 10
MAX_COMMENT_PAGES = 10

# ã‚³ãƒ¡ãƒ³ãƒˆç·æ•°ã®ä¸Šé™ï¼ˆãã‚Œä»¥ä¸Šã‚ã‚Œã°ã€Œâ€¦(over 3000)ã€ãªã©ã¨è¿½è¨˜ï¼‰
MAX_COMMENTS_TOTAL = 3000

# Yahoo ã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€å®šç¾©
# A:URL, B:ã‚¿ã‚¤ãƒˆãƒ«, C:æŠ•ç¨¿æ—¥æ™‚, D:ã‚½ãƒ¼ã‚¹,
# Eã€œN: æœ¬æ–‡(1ã€œ10ãƒšãƒ¼ã‚¸), O:ã‚³ãƒ¡ãƒ³ãƒˆæ•°,
# P:ä¸»é¡Œä¼æ¥­, Q:ã‚«ãƒ†ã‚´ãƒª, R:ãƒã‚¸ãƒã‚¬, S:æ—¥ç”£é–¢é€£æ–‡, T:æ—¥ç”£ãƒã‚¬æ–‡
YAHOO_SHEET_HEADERS = [
    "URL",          # A
    "ã‚¿ã‚¤ãƒˆãƒ«",      # B
    "æŠ•ç¨¿æ—¥æ™‚",      # C
    "ã‚½ãƒ¼ã‚¹",        # D
    "æœ¬æ–‡_P1",      # E
    "æœ¬æ–‡_P2",      # F
    "æœ¬æ–‡_P3",      # G
    "æœ¬æ–‡_P4",      # H
    "æœ¬æ–‡_P5",      # I
    "æœ¬æ–‡_P6",      # J
    "æœ¬æ–‡_P7",      # K
    "æœ¬æ–‡_P8",      # L
    "æœ¬æ–‡_P9",      # M
    "æœ¬æ–‡_P10",     # N
    "ã‚³ãƒ¡ãƒ³ãƒˆæ•°",     # O
    "ä¸»é¡Œä¼æ¥­",       # P
    "ã‚«ãƒ†ã‚´ãƒª",       # Q
    "ãƒã‚¸ãƒã‚¬",       # R
    "æ—¥ç”£é–¢é€£æ–‡",     # S
    "æ—¥ç”£ãƒã‚¬æ–‡"      # T
]

# Comments ã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€å®šç¾©
# A:URL, B:ã‚¿ã‚¤ãƒˆãƒ«, C:æŠ•ç¨¿æ—¥æ™‚, D:ã‚½ãƒ¼ã‚¹, E:ã‚³ãƒ¡ãƒ³ãƒˆæ•°, Fã€œ:ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸
COMMENTS_SHEET_HEADERS = [
    "URL",          # A
    "ã‚¿ã‚¤ãƒˆãƒ«",      # B
    "æŠ•ç¨¿æ—¥æ™‚",      # C
    "ã‚½ãƒ¼ã‚¹",        # D
    "ã‚³ãƒ¡ãƒ³ãƒˆæ•°"      # E
    # Fã€œ: ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸1ã€œ10
]

REQ_HEADERS = {"User-Agent": "Mozilla/5.0"}
TZ_JST = timezone(timedelta(hours=9))

PROMPT_FILES = [
    "prompt_gemini_role.txt",
    "prompt_posinega.txt",
    "prompt_category.txt",
    "prompt_target_company.txt"
]

# ===== Gemini Client åˆæœŸåŒ– =====
try:
    GEMINI_CLIENT = genai.Client()
except Exception as e:
    print(f"è­¦å‘Š: Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Geminiåˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    GEMINI_CLIENT = None

GEMINI_BATCH_PROMPT_BASE = None  # ãƒãƒƒãƒç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

# ================== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ==================

def jst_now() -> datetime:
    return datetime.now(TZ_JST)

def format_datetime(dt_obj: datetime) -> str:
    """yyyy/mm/dd hh:mm:ss å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    return dt_obj.strftime("%Y/%m/%d %H:%M:%S")

def parse_post_date(raw, today_jst: datetime) -> Optional[datetime]:
    """Yahooè¡¨ç¤ºã®æ—¥æ™‚æ–‡å­—åˆ—ã‚’ datetime(JST) ã«å¤‰æ›"""
    if raw is None:
        return None
    if isinstance(raw, (int, float)):
        # ã‚·ãƒ¼ãƒˆå†…ã§æ—¢ã«æ—¥ä»˜å‹ã«ãªã£ã¦ã„ã‚‹å ´åˆã‚‚ã‚ã‚Šå¾—ã‚‹
        try:
            # gspreadã¯ã‚·ãƒªã‚¢ãƒ«å€¤ã§è¿”ã•ãªã„ã®ã§åŸºæœ¬é€šã‚‰ãªã„æƒ³å®š
            return None
        except Exception:
            return None

    if isinstance(raw, str):
        s = raw.strip()

        # (æœˆ) ãªã©ã®æ›œæ—¥ã‚’å‰Šé™¤
        s = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)", "", s).strip()
        # ã€Œé…ä¿¡ã€ã‚’å‰Šé™¤
        s = s.replace("é…ä¿¡", "").strip()

        # ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é †ã«è©¦ã™
        for fmt in ("%Y/%m/%d %H:%M:%S", "%Y/%m/%d %H:%M", "%y/%m/%d %H:%M", "%m/%d %H:%M"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m/%d %H:%M":
                    dt = dt.replace(year=today_jst.year)
                # æœªæ¥ã™ãã‚‹å ´åˆã¯å‰å¹´ã«è£œæ­£
                if dt.replace(tzinfo=TZ_JST) > today_jst + timedelta(days=31):
                    dt = dt.replace(year=dt.year - 1)
                return dt.replace(tzinfo=TZ_JST)
            except ValueError:
                continue

    return None

def build_gspread_client() -> gspread.Client:
    """ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ gspread ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆ"""
    try:
        creds_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive"
        ]
        if creds_str:
            info = json.loads(creds_str)
            credentials = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
            return gspread.authorize(credentials)
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ« fallback
            return gspread.service_account(filename="credentials.json")
    except FileNotFoundError:
        raise RuntimeError("Googleèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ã¾ãŸã¯ credentials.json ã‚’ç¢ºèª)")
    except Exception as e:
        raise RuntimeError(f"Googleèªè¨¼ã«å¤±æ•—: {e}")

def load_keywords(filename: str) -> List[str]:
    """keywords.txt ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿"""
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(script_dir, filename)
        with open(file_path, "r", encoding="utf-8") as f:
            keywords = [line.strip() for line in f if line.strip() and not line.startswith("#")]
        if not keywords:
            raise ValueError("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã«æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return keywords
    except FileNotFoundError:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return []
    except Exception as e:
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def load_gemini_batch_prompt() -> str:
    """
    PROMPT_FILES ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒƒãƒåˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆã€‚
    10ä»¶ã¾ã¨ã‚ã¦ JSON é…åˆ—ã§è¿”ã™ã‚ˆã†ã«æŒ‡ç¤ºã™ã‚‹ã€‚
    """
    global GEMINI_BATCH_PROMPT_BASE
    if GEMINI_BATCH_PROMPT_BASE is not None:
        return GEMINI_BATCH_PROMPT_BASE

    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))

        # å½¹å‰²ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        role_path = os.path.join(script_dir, PROMPT_FILES[0])
        with open(role_path, "r", encoding="utf-8") as f:
            role_instruction = f.read().strip()

        other_contents = []
        for filename in PROMPT_FILES[1:]:
            path = os.path.join(script_dir, filename)
            with open(path, "r", encoding="utf-8") as f:
                c = f.read().strip()
                if c:
                    other_contents.append(c)

        if not role_instruction or not other_contents:
            print("è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒä¸å®Œå…¨ã¾ãŸã¯ç©ºã§ã™ã€‚")
            GEMINI_BATCH_PROMPT_BASE = ""
            return GEMINI_BATCH_PROMPT_BASE

        base = role_instruction + "\n" + "\n".join(other_contents)

        # è¿½åŠ ã®æ˜ç¤ºçš„ãªæŒ‡ç¤ºï¼ˆãƒãƒƒãƒ & æ—¥ç”£æŠ½å‡ºï¼‰
        extra = """
è¿½åŠ è¦ä»¶:
- ä¸ãˆã‚‰ã‚Œã‚‹è¨˜äº‹æœ¬æ–‡ã¯æœ€å¤§ã§10ä»¶ã§ã™ã€‚
- ãã‚Œãã‚Œã®è¨˜äº‹ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
  * company_info: è¨˜äº‹ã®ä¸»é¡Œä¼æ¥­åã€‚å…±åŒé–‹ç™ºãªã©ãŒã‚ã‚Œã° () å†…ã«åˆ¥ä¼æ¥­ã‚‚æ›¸ã„ã¦ãã ã•ã„ã€‚
  * category: ä¼æ¥­ã€ãƒ¢ãƒ‡ãƒ«ã€æŠ€è¡“ã€ç¤¾ä¼šã€æŠ•è³‡ãªã©ã€PROMPTã§æŒ‡å®šã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã«å¾“ã£ã¦ãã ã•ã„ã€‚
  * sentiment: è¨˜äº‹å…¨ä½“ã®ãƒˆãƒ¼ãƒ³ã‚’ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ã®ã„ãšã‚Œã‹ã§åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
  * nissan_related: è¨˜äº‹æœ¬æ–‡ä¸­ã§ã€Œæ—¥ç”£ã€ã‚„ã€ŒNISSANã€ã€Œãƒ‹ãƒƒã‚µãƒ³ã€ãªã©ã€æ—¥ç”£è‡ªå‹•è»Šã‚„ãã®å•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã«è¨€åŠã—ã¦ã„ã‚‹æ–‡ã‚’ã€æ—¥æœ¬èªã®æ–‡ç« ã¨ã—ã¦å¯èƒ½ãªé™ã‚ŠæŠ½å‡ºã—ã¦ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚ãªã‘ã‚Œã° "N/A" ã¨ã—ã¦ãã ã•ã„ã€‚
  * nissan_negative: ä¸Šè¨˜ nissan_related ã®æ–‡ã®ã†ã¡ã€æ—¥ç”£ã‚„æ—¥ç”£ã®å•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã«å¯¾ã—ã¦ãƒã‚¬ãƒ†ã‚£ãƒ–ãªå°è±¡ã‚’ä¸ãˆã‚‹å†…å®¹ï¼ˆæ‰¹åˆ¤ãƒ»ä¸æº€ãƒ»æ‡¸å¿µãªã©ï¼‰ã ã‘ã‚’æŠ½å‡ºã—ã¦ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚ãªã‘ã‚Œã° "N/A" ã¨ã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
- å¿…ãš JSON é…åˆ—å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
- é…åˆ—ã®å„è¦ç´ ã¯ã€æ¬¡ã®ã‚­ãƒ¼ã‚’æŒã¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¾ã™:
  {
    "index": 0,  // å…¥åŠ›é †ã« 0,1,2,... ã¨ã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    "company_info": "string",
    "category": "string",
    "sentiment": "string",
    "nissan_related": "string",
    "nissan_negative": "string"
  }

å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
- ä»¥ä¸‹ã®ã‚ˆã†ã«ã€è¨˜äº‹æœ¬æ–‡ãŒè¤‡æ•°èˆ‡ãˆã‚‰ã‚Œã¾ã™ã€‚
- "==== ARTICLE i START ====" ã¨ "==== ARTICLE i END ====" ã«æŒŸã¾ã‚ŒãŸéƒ¨åˆ†ãŒã€index=i ã®è¨˜äº‹æœ¬æ–‡ã§ã™ã€‚

å®Ÿè¡Œã‚¿ã‚¹ã‚¯:
- å„è¨˜äº‹ã”ã¨ã«ã€ä¸Šè¨˜ã® JSON ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«ä¸¦ã¹ãŸ JSON é…åˆ—ã‚’1ã¤ã ã‘å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

        base += "\n\n" + extra + "\n\n{TEXT_BATCH}"

        GEMINI_BATCH_PROMPT_BASE = base
        print(f"Gemini ãƒãƒƒãƒç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ {PROMPT_FILES} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return GEMINI_BATCH_PROMPT_BASE

    except FileNotFoundError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«å: {e.filename}")
        GEMINI_BATCH_PROMPT_BASE = ""
        return GEMINI_BATCH_PROMPT_BASE
    except Exception as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        GEMINI_BATCH_PROMPT_BASE = ""
        return GEMINI_BATCH_PROMPT_BASE

def request_with_retry(url: str, max_retries: int = 3) -> Optional[requests.Response]:
    """requests.get ã‚’ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§å®Ÿè¡Œã€‚404 ã®å ´åˆã¯å³ None"""
    for attempt in range(max_retries):
        try:
            res = requests.get(url, headers=REQ_HEADERS, timeout=20)
            if res.status_code == 404:
                print(f"  âŒ 404 Not Found: {url}")
                return None
            res.raise_for_status()
            return res
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt + random.random()
                print(f"  âš ï¸ æ¥ç¶šã‚¨ãƒ©ãƒ¼, ãƒªãƒˆãƒ©ã‚¤ {attempt+1}/{max_retries} ({wait_time:.2f}ç§’): {e}")
                time.sleep(wait_time)
            else:
                print(f"  âŒ æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤å¤±æ•—: {e}")
                return None
    return None

# ================== Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ (Selenium) ==================

def get_yahoo_news_with_selenium(keyword: str) -> List[Dict[str, str]]:
    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword})...")
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument(f"user-agent={REQ_HEADERS['User-Agent']}")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)

    # GitHub Actions ã§ã¯ PATH ä¸Šã® chromedriver ã‚’ä½¿ã†æƒ³å®š
    try:
        driver = webdriver.Chrome(options=options)
    except Exception as e:
        print(f" WebDriver åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return []

    search_url = (
        f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
        "&categories=domestic,world,business,it,science,life,local"
    )
    driver.get(search_url)

    try:
        WebDriverWait(driver, 20).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "li[class*='sc-1u4589e-0']"))
        )
        time.sleep(3)
    except Exception as e:
        print(f"  âš ï¸ ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰/è¦ç´ å¾…ã¡ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {e}")
        time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    articles_data: List[Dict[str, str]] = []
    today_jst = jst_now()

    for article in articles:
        try:
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            link_tag = article.find("a", href=True)
            url = (
                link_tag["href"]
                if link_tag and link_tag["href"].startswith("https://news.yahoo.co.jp/articles/")
                else ""
            )

            time_tag = article.find("time")
            date_str = time_tag.text.strip() if time_tag else ""

            # ã‚½ãƒ¼ã‚¹æŠ½å‡ºï¼ˆæ§‹é€ ã¯é »ç¹ã«å¤‰ã‚ã‚‹ãŸã‚ã€é•·ã‚ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¡ç”¨ï¼‰
            source_text = ""
            source_container = article.find("div", class_=re.compile("sc-n3vj8g-0"))
            if source_container:
                time_and_comments = source_container.find("div", class_=re.compile("sc-110wjhy-8"))
                if time_and_comments:
                    spans = [
                        s.text.strip()
                        for s in time_and_comments.find_all("span")
                        if not s.find("svg")
                    ]
                    # æ—¥ä»˜ã‚‰ã—ãã‚‚ã®ã¯é™¤å»
                    spans = [
                        s
                        for s in spans
                        if not re.match(r"\d{1,2}/\d{1,2}.*\d{1,2}:\d{2}", s)
                    ]
                    if spans:
                        source_text = max(spans, key=len)

            formatted_date = "å–å¾—ä¸å¯"
            if date_str:
                dt_obj = parse_post_date(date_str, today_jst)
                if dt_obj:
                    formatted_date = format_datetime(dt_obj)
                else:
                    formatted_date = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)", "", date_str).strip()

            if title and url:
                articles_data.append(
                    {
                        "URL": url,
                        "ã‚¿ã‚¤ãƒˆãƒ«": title,
                        "æŠ•ç¨¿æ—¥æ™‚": formatted_date,
                        "ã‚½ãƒ¼ã‚¹": source_text or "å–å¾—ä¸å¯",
                    }
                )
        except Exception:
            continue

    print(f"  Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(articles_data)} ä»¶å–å¾—")
    return articles_data

# ================== ã‚·ãƒ¼ãƒˆæ“ä½œãƒ˜ãƒ«ãƒ‘ãƒ¼ ==================

def ensure_yahoo_sheet(gc: gspread.Client) -> gspread.Worksheet:
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(SOURCE_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(
            title=SOURCE_SHEET_NAME,
            rows=str(MAX_SHEET_ROWS_FOR_REPLACE),
            cols=str(len(YAHOO_SHEET_HEADERS)),
        )
    headers = ws.row_values(1)
    if headers != YAHOO_SHEET_HEADERS:
        ws.update(
            range_name=f"A1:{gspread.utils.rowcol_to_a1(1, len(YAHOO_SHEET_HEADERS))}",
            values=[YAHOO_SHEET_HEADERS],
        )
    return ws

def ensure_comments_sheet(gc: gspread.Client) -> gspread.Worksheet:
    sh = gc.open_by_key(SOURCE_SPREADSHEET_ID)
    try:
        ws = sh.worksheet(COMMENTS_SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(
            title=COMMENTS_SHEET_NAME,
            rows=str(MAX_SHEET_ROWS_FOR_REPLACE),
            cols=str(5 + MAX_COMMENT_PAGES),
        )
        ws.update(
            range_name=f"A1:{gspread.utils.rowcol_to_a1(1, 5 + MAX_COMMENT_PAGES)}",
            values=[COMMENTS_SHEET_HEADERS + [f"ã‚³ãƒ¡ãƒ³ãƒˆ_P{i}" for i in range(1, MAX_COMMENT_PAGES + 1)]],
        )
    return ws

def write_news_list_to_source(gc: gspread.Client, articles: List[Dict[str, str]]) -> None:
    ws = ensure_yahoo_sheet(gc)
    existing = ws.get_all_values(value_render_option="UNFORMATTED_VALUE")
    existing_urls = set(
        str(row[0])
        for row in existing[1:]
        if len(row) > 0 and str(row[0]).startswith("http")
    )

    new_rows = []
    for a in articles:
        if a["URL"] in existing_urls:
            continue
        row = [
            a["URL"],
            a["ã‚¿ã‚¤ãƒˆãƒ«"],
            a["æŠ•ç¨¿æ—¥æ™‚"],
            a["ã‚½ãƒ¼ã‚¹"],
        ]
        # æ®‹ã‚Šã®åˆ—ã¯ç©ºã§åŸ‹ã‚ã‚‹
        row.extend([""] * (len(YAHOO_SHEET_HEADERS) - len(row)))
        new_rows.append(row)

    if new_rows:
        ws.append_rows(new_rows, value_input_option="USER_ENTERED")
        print(f"  Yahooã‚·ãƒ¼ãƒˆã« {len(new_rows)} ä»¶è¿½è¨˜ã—ã¾ã—ãŸã€‚")
    else:
        print("  è¿½è¨˜ã™ã¹ãæ–°è¦è¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

# ================== æœ¬æ–‡ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— ==================

def fetch_article_body_page(url: str) -> str:
    """
    è¨˜äº‹ã®å˜ä¸€ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    """
    res = request_with_retry(url)
    if not res:
        return ""

    soup = BeautifulSoup(res.text, "html.parser")

    # articleã‚¿ã‚°ã‚„ article_bodyç›¸å½“ã‚’æ¢ã™
    article_content = (
        soup.find("article")
        or soup.find("div", class_="article_body")
        or soup.find("div", class_=re.compile(r"article_detail|article_body"))
    )

    texts: List[str] = []
    if article_content:
        # ãƒã‚¤ãƒ©ã‚¤ãƒˆå¯¾è±¡ã® p ã‹ã‚‰å„ªå…ˆçš„ã«å–å¾—
        paragraphs = article_content.find_all(
            "p", class_=re.compile(r"highLightSearchTarget")
        )
        if not paragraphs:
            paragraphs = article_content.find_all("p")
        for p in paragraphs:
            t = p.get_text(strip=True)
            if t:
                texts.append(t)

    return "\n".join(texts).strip()

def fetch_comments_page(url: str) -> List[str]:
    """
    ã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸1æšåˆ†ã‹ã‚‰ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã‚’ãƒªã‚¹ãƒˆã§è¿”ã™ã€‚
    ï¼ˆYahooã®HTMLæ§‹é€ ã¯é »ç¹ã«å¤‰ã‚ã‚‹ãŸã‚ã€æ±ç”¨çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ã§å–å¾—ï¼‰
    """
    res = request_with_retry(url)
    if not res:
        return []

    soup = BeautifulSoup(res.text, "html.parser")

    comments: List[str] = []

    # ä»£è¡¨çš„ãªã‚¯ãƒ©ã‚¹åã‚’ã„ãã¤ã‹è©¦ã—ãªãŒã‚‰ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã£ã½ã„è¦ç´ ã‚’æ‹¾ã†
    candidate_selectors = [
        "div[class*='CommentItem__body']",
        "p[class*='CommentItem__body']",
        "span[class*='CommentItem__body']",
        "p[class*='sc-']",
    ]
    for sel in candidate_selectors:
        for node in soup.select(sel):
            text = node.get_text(strip=True)
            # ã‚ã‚‹ç¨‹åº¦ã®é•·ã•ãŒã‚ã‚‹ã‚‚ã®ã ã‘
            if text and len(text) > 5 and text not in comments:
                comments.append(text)

    # å€™è£œãŒå°‘ãªã™ãã‚‹å ´åˆã¯ã€ã‚ˆã‚Šç·©ã„æŠ½å‡ºã¯è¡Œã‚ãšã€ãã®ã¾ã¾è¿”ã™
    return comments

def fetch_details_and_update(gc: gspread.Client) -> None:
    """
    Yahooã‚·ãƒ¼ãƒˆã®å„è¡Œã«ã¤ã„ã¦:
      - æœ¬æ–‡10ãƒšãƒ¼ã‚¸åˆ† (Eã€œN) ã‚’å–å¾—ãƒ»æ›´æ–°
      - ã‚³ãƒ¡ãƒ³ãƒˆæ•°(O) ã‚’æ›´æ–°
      - Commentsã‚·ãƒ¼ãƒˆã«ã‚³ãƒ¡ãƒ³ãƒˆ10ãƒšãƒ¼ã‚¸åˆ†ã‚’æ›¸ãè¾¼ã¿
    æ—¢ã«æœ¬æ–‡P1ãŒå…¥ã£ã¦ã„ã‚‹è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå†å–å¾—ã—ãªã„ï¼‰
    """
    yahoo_ws = ensure_yahoo_sheet(gc)
    comments_ws = ensure_comments_sheet(gc)

    values = yahoo_ws.get_all_values(value_render_option="UNFORMATTED_VALUE")
    if len(values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æœ¬æ–‡/ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    data_rows = values[1:]
    print("\n===== ğŸ“„ ã‚¹ãƒ†ãƒƒãƒ—2: æœ¬æ–‡ï¼†ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ãƒ»Commentsã‚·ãƒ¼ãƒˆæ›´æ–° =====")

    # Commentsã‚·ãƒ¼ãƒˆå´ã®URLâ†’rowç•ªå·ãƒãƒƒãƒ—ã‚’å…ˆã«ä½œã£ã¦ãŠã
    comments_values = comments_ws.get_all_values(value_render_option="UNFORMATTED_VALUE")
    comments_url_to_row: Dict[str, int] = {}
    if comments_values:
        for idx, row in enumerate(comments_values[1:], start=2):
            if len(row) > 0 and row[0].startswith("http"):
                comments_url_to_row[row[0]] = idx

    update_body_count = 0
    update_comments_count = 0

    for idx, row in enumerate(data_rows, start=2):
        # è¡Œé•·ã‚’ãƒ˜ãƒƒãƒ€é•·ã«åˆã‚ã›ã‚‹
        if len(row) < len(YAHOO_SHEET_HEADERS):
            row.extend([""] * (len(YAHOO_SHEET_HEADERS) - len(row)))

        url = row[0].strip()
        if not url.startswith("https://news.yahoo.co.jp/articles/"):
            continue

        title = row[1]
        post_date = row[2]
        source = row[3]

        # æœ¬æ–‡åˆ— (Eã€œN)
        body_pages = row[4:4 + MAX_BODY_PAGES]
        # ã‚³ãƒ¡ãƒ³ãƒˆæ•° (O)
        comment_count_str = row[14].strip() if len(row) > 14 else ""

        # æœ¬æ–‡P1 ãŒç©ºã®å ´åˆã®ã¿æœ¬æ–‡å–å¾—ã‚’å®Ÿæ–½ï¼ˆç„¡é™å†å–å¾—é˜²æ­¢ï¼‰
        need_body = not body_pages[0].strip()

        # ã‚³ãƒ¡ãƒ³ãƒˆé–¢é€£ã¯éƒ½åº¦æ›´æ–°ï¼ˆå¤ã„ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’æ®‹ã—ãŸããªã„ãŸã‚ï¼‰
        need_comments = True

        new_body_pages = list(body_pages)
        new_comment_count = comment_count_str

        if need_body or need_comments:
            print(f"  - è¡Œ {idx} (è¨˜äº‹: {title[:20]}...): è©³ç´°å–å¾—ä¸­...")

        # ===== æœ¬æ–‡å–å¾— (æœ€å¤§10ãƒšãƒ¼ã‚¸) =====
        if need_body:
            body_changed = False
            for page_idx in range(1, MAX_BODY_PAGES + 1):
                if page_idx == 1:
                    page_url = url
                else:
                    page_url = f"{url}?page={page_idx}"

                text = fetch_article_body_page(page_url)
                if not text:
                    # 2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã§æœ¬æ–‡ãŒç©º => ä»¥é™ã®ãƒšãƒ¼ã‚¸ã¯å­˜åœ¨ã—ãªã„ã¨ã¿ãªã—ã¦ break
                    if page_idx == 1:
                        # 1ãƒšãƒ¼ã‚¸ç›®ã™ã‚‰å–ã‚Œãªã„å ´åˆ
                        new_body_pages[0] = "æœ¬æ–‡å–å¾—ä¸å¯"
                        body_changed = True
                    break

                col_idx = page_idx - 1  # 0~9
                if new_body_pages[col_idx] != text:
                    new_body_pages[col_idx] = text
                    body_changed = True

            # æ›´æ–°åæ˜ 
            if body_changed:
                yahoo_ws.update(
                    range_name=f"E{idx}:N{idx}",
                    values=[new_body_pages],
                    value_input_option="USER_ENTERED",
                )
                update_body_count += 1
                # ã‚·ãƒ¼ãƒˆ API è² è·å¯¾ç­–
                time.sleep(1 + random.random() * 0.5)

        # ===== ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— (æœ€å¤§10ãƒšãƒ¼ã‚¸) =====
        if need_comments:
            all_comments: List[str] = []
            page_strings: List[str] = [""] * MAX_COMMENT_PAGES

            for page_idx in range(1, MAX_COMMENT_PAGES + 1):
                if page_idx == 1:
                    c_url = url + "/comments"
                else:
                    c_url = url + f"/comments?page={page_idx}"

                comments = fetch_comments_page(c_url)
                if not comments:
                    # ã‚³ãƒ¡ãƒ³ãƒˆãŒã¾ã£ãŸãå–ã‚Œãªã„ãƒšãƒ¼ã‚¸ãŒæ¥ãŸã‚‰ä»¥é™ã¯çµ‚äº†
                    break

                # ç·æ•°åˆ¶é™
                if len(all_comments) + len(comments) > MAX_COMMENTS_TOTAL:
                    remaining = MAX_COMMENTS_TOTAL - len(all_comments)
                    if remaining > 0:
                        comments = comments[:remaining]
                    all_comments.extend(comments)
                    # ã“ã®ãƒšãƒ¼ã‚¸ã®ã‚»ãƒ«ç”¨æ–‡å­—åˆ—
                    numbered = []
                    for i, c in enumerate(comments, start=1):
                        numbered.append(f"[{i}] {c}")
                    page_strings[page_idx - 1] = "\n\n".join(numbered) + "\n\n(â€» over 3000 comments, truncated)"
                    # 3000ã‚’è¶…ãˆãŸã®ã§çµ‚äº†
                    break
                else:
                    all_comments.extend(comments)
                    numbered = []
                    for i, c in enumerate(comments, start=1):
                        numbered.append(f"[{i}] {c}")
                    page_strings[page_idx - 1] = "\n\n".join(numbered)

            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°æ›´æ–°
            new_comment_count = str(len(all_comments))

            # Yahooã‚·ãƒ¼ãƒˆå´ Oåˆ—
            yahoo_ws.update(
                range_name=f"O{idx}:O{idx}",
                values=[[new_comment_count]],
                value_input_option="USER_ENTERED",
            )
            update_comments_count += 1

            # Commentsã‚·ãƒ¼ãƒˆå´
            if url in comments_url_to_row:
                c_row = comments_url_to_row[url]
                # æ—¢å­˜è¡Œã‚’ä¸Šæ›¸ã
                base_vals = [url, title, post_date, source, new_comment_count]
                base_vals.extend(page_strings)
                comments_ws.update(
                    range_name=f"A{c_row}:{gspread.utils.rowcol_to_a1(c_row, 5 + MAX_COMMENT_PAGES)}",
                    values=[base_vals],
                    value_input_option="USER_ENTERED",
                )
            else:
                # æ–°è¦è¡Œã¨ã—ã¦æœ«å°¾ã«è¿½åŠ 
                base_vals = [url, title, post_date, source, new_comment_count]
                base_vals.extend(page_strings)
                comments_ws.append_row(base_vals, value_input_option="USER_ENTERED")
                new_row_index = len(comments_values) + 1 + len(comments_url_to_row)  # ã–ã£ãã‚Š
                comments_url_to_row[url] = new_row_index

            time.sleep(1 + random.random() * 0.5)

    print(f" âœ… æœ¬æ–‡å–å¾—ã‚’ {update_body_count} è¡Œã«å®Ÿè¡Œ")
    print(f" âœ… ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ï¼†Commentsã‚·ãƒ¼ãƒˆæ›´æ–°ã‚’ {update_comments_count} è¡Œã«å®Ÿè¡Œ")

# ================== Gemini ãƒãƒƒãƒåˆ†æ ==================

def analyze_with_gemini_batch(texts: List[str]) -> List[Dict[str, str]]:
    """
    æœ€å¤§10ä»¶ã®æœ¬æ–‡ã‚’ã¾ã¨ã‚ã¦ Gemini ã§åˆ†æã—ã€JSONé…åˆ—ã‚’è¿”ã™ã€‚
    texts[i] ãŒ index=i ã«å¯¾å¿œã€‚
    """
    if not GEMINI_CLIENT:
        return []

    if not texts:
        return []

    prompt_template = load_gemini_batch_prompt()
    if not prompt_template:
        print("Geminiãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºã®ãŸã‚ã€ãƒãƒƒãƒåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []

    # é•·ã•ã‚’åˆ¶é™ï¼ˆ1ä»¶ã‚ãŸã‚Š15,000æ–‡å­—ã¾ã§ï¼‰
    trimmed_texts = [t[:15000] for t in texts]

    # {TEXT_BATCH} ã‚’ç”Ÿæˆ
    blocks = []
    for i, txt in enumerate(trimmed_texts):
        blocks.append(
            f"==== ARTICLE {i} START ====\n{txt}\n==== ARTICLE {i} END ===="
        )
    text_batch = "\n\n".join(blocks)

    prompt = prompt_template.replace("{TEXT_BATCH}", text_batch)

    MAX_RETRIES = 3
    for attempt in range(MAX_RETRIES):
        try:
            resp = GEMINI_CLIENT.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                },
            )
            raw = resp.text.strip()
            try:
                data = json.loads(raw)
            except json.JSONDecodeError:
                print("Gemini ã‹ã‚‰ã® JSON ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è¿”å´ãƒ†ã‚­ã‚¹ãƒˆ:")
                print(raw)
                return []

            if isinstance(data, dict) and "results" in data:
                results = data["results"]
            elif isinstance(data, list):
                results = data
            else:
                print("Geminiå¿œç­”JSONå½¢å¼ãŒæƒ³å®šå¤–: dict(results=...) ã¾ãŸã¯ list[...] ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                return []

            out: List[Dict[str, str]] = []
            for item in results:
                if not isinstance(item, dict):
                    continue
                out.append(
                    {
                        "index": item.get("index", 0),
                        "company_info": item.get("company_info", "N/A"),
                        "category": item.get("category", "N/A"),
                        "sentiment": item.get("sentiment", "N/A"),
                        "nissan_related": item.get("nissan_related", "N/A"),
                        "nissan_negative": item.get("nissan_negative", "N/A"),
                    }
                )
            return out

        except ResourceExhausted as e:
            # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ -> ã“ã“ã§å…¨ä½“å‡¦ç†ã‚’çµ‚äº†ã•ã›ã‚‹ï¼ˆå‘¼ã³å‡ºã—å´ã§æ¤œçŸ¥ï¼‰
            print(f"  ğŸš¨ Gemini API ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã‚¨ãƒ©ãƒ¼ (429): {e}")
            raise
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                wait = 2 ** attempt + random.random()
                print(f"  âš ï¸ Gemini API ä¸€æ™‚ã‚¨ãƒ©ãƒ¼ã€‚{wait:.2f}ç§’å¾Œã«å†è©¦è¡Œ ({attempt+1}/{MAX_RETRIES}) | {e}")
                time.sleep(wait)
            else:
                print(f"Geminiãƒãƒƒãƒåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                return []

    return []

def analyze_and_update_sheet(gc: gspread.Client) -> None:
    """
    Yahooã‚·ãƒ¼ãƒˆã®æœ¬æ–‡ (Eã€œN) ãŒå…¥ã£ã¦ã„ã‚‹è¡Œã§ã€
      - P:ä¸»é¡Œä¼æ¥­
      - Q:ã‚«ãƒ†ã‚´ãƒª
      - R:ãƒã‚¸ãƒã‚¬
      - S:æ—¥ç”£é–¢é€£æ–‡
      - T:æ—¥ç”£ãƒã‚¬æ–‡
    ãŒç©ºã®ã‚‚ã®ã‚’å¯¾è±¡ã«ã€æœ€å¤§10ä»¶ã¾ã¨ã‚ã¦ Gemini ã§åˆ†æï¼†æ›´æ–°ã™ã‚‹ã€‚
    """
    if not GEMINI_CLIENT:
        print("Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    ws = ensure_yahoo_sheet(gc)
    values = ws.get_all_values(value_render_option="UNFORMATTED_VALUE")
    if len(values) <= 1:
        print(" Yahooã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€Geminiåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    data_rows = values[1:]

    print("\n===== ğŸ§  ã‚¹ãƒ†ãƒƒãƒ—3: Geminiãƒãƒƒãƒåˆ†æ (10ä»¶/1API) =====")

    # åˆ†æå¯¾è±¡è¡Œã‚’åé›†
    targets: List[Dict[str, Any]] = []
    for idx, row in enumerate(data_rows, start=2):
        if len(row) < len(YAHOO_SHEET_HEADERS):
            row.extend([""] * (len(YAHOO_SHEET_HEADERS) - len(row)))

        url = row[0].strip()
        if not url.startswith("https://news.yahoo.co.jp/articles/"):
            continue

        # æœ¬æ–‡ãƒšãƒ¼ã‚¸ã‚’çµåˆ
        pages = row[4:4 + MAX_BODY_PAGES]
        pages = [p for p in pages if p and p != "æœ¬æ–‡å–å¾—ä¸å¯"]
        if not pages:
            continue
        body_text = "\n\n".join(
            f"ã€Page{i+1}ã€‘\n{p}" for i, p in enumerate(pages)
        )

        # ã™ã§ã« Pã€œT ãŒã™ã¹ã¦åŸ‹ã¾ã£ã¦ã„ã‚‹è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
        company_info = row[15] if len(row) > 15 else ""
        category = row[16] if len(row) > 16 else ""
        sentiment = row[17] if len(row) > 17 else ""
        nissan_rel = row[18] if len(row) > 18 else ""
        nissan_neg = row[19] if len(row) > 19 else ""
        if company_info and category and sentiment and nissan_rel and nissan_neg:
            continue

        targets.append(
            {
                "row_index": idx,
                "url": url,
                "title": row[1],
                "body": body_text,
            }
        )

    if not targets:
        print("  Geminiåˆ†æãŒå¿…è¦ãªè¡Œã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"  Geminiåˆ†æå¯¾è±¡: {len(targets)} è¡Œ")

    updated_count = 0
    # 10ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†
    for i in range(0, len(targets), 10):
        batch = targets[i : i + 10]
        texts = [t["body"] for t in batch]
        print(f"  - {i+1}ã€œ{i+len(batch)}ä»¶ç›®ã‚’ãƒãƒƒãƒåˆ†æä¸­...")

        try:
            results = analyze_with_gemini_batch(texts)
        except ResourceExhausted:
            # ã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ãŒå‡ºãŸã‚‰ã“ã‚Œä»¥ä¸Šã®åˆ†æã¯ã‚ãã‚‰ã‚ã‚‹
            print("  ğŸš¨ Geminiã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«åˆ°é”ã—ãŸãŸã‚ã€æ®‹ã‚Šã®åˆ†æã¯æ¬¡å›ã¸æŒã¡è¶Šã—ã¾ã™ã€‚")
            break

        # index ã§ç´ã¥ã‘
        result_by_index: Dict[int, Dict[str, str]] = {}
        for r in results:
            try:
                idx_int = int(r.get("index", 0))
            except Exception:
                idx_int = 0
            result_by_index[idx_int] = r

        # ãƒãƒƒãƒå†…å„è¡Œã‚’æ›´æ–°
        for local_idx, item in enumerate(batch):
            row_idx = item["row_index"]
            r = result_by_index.get(local_idx)
            if not r:
                # å¯¾å¿œã™ã‚‹çµæœãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                continue

            company_info = r.get("company_info", "N/A")
            category = r.get("category", "N/A")
            sentiment = r.get("sentiment", "N/A")
            nissan_rel = r.get("nissan_related", "N/A")
            nissan_neg = r.get("nissan_negative", "N/A")

            ws.update(
                range_name=f"P{row_idx}:T{row_idx}",
                values=[[company_info, category, sentiment, nissan_rel, nissan_neg]],
                value_input_option="USER_ENTERED",
            )
            updated_count += 1
            time.sleep(0.8 + random.random() * 0.4)

    print(f" âœ… Geminiãƒãƒƒãƒåˆ†æçµæœã‚’ {updated_count} è¡Œã«åæ˜ ã—ã¾ã—ãŸã€‚")

# ================== ãƒ¡ã‚¤ãƒ³å‡¦ç† ==================

def main():
    print("--- Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼é–‹å§‹ ---")

    keywords = load_keywords(KEYWORD_FILE)
    if not keywords:
        print("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„ãŸã‚çµ‚äº†ã—ã¾ã™ã€‚")
        sys.exit(0)

    try:
        gc = build_gspread_client()
    except RuntimeError as e:
        print(f"è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã—ã¦ Yahoo ã‚·ãƒ¼ãƒˆã«è¿½è¨˜
    for kw in keywords:
        print(f"\n===== ğŸ”‘ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆå–å¾—: {kw} =====")
        articles = get_yahoo_news_with_selenium(kw)
        write_news_list_to_source(gc, articles)
        time.sleep(2)

    # ã‚¹ãƒ†ãƒƒãƒ—2: æœ¬æ–‡ï¼†ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— + Commentsã‚·ãƒ¼ãƒˆæ›´æ–°
    fetch_details_and_update(gc)

    # ã‚¹ãƒ†ãƒƒãƒ—3: Geminiãƒãƒƒãƒåˆ†æ
    analyze_and_update_sheet(gc)

    print("\n--- Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Œäº† ---")


if __name__ == "__main__":
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ sys.path ã«è¿½åŠ ï¼ˆPROMPT_FILES èª­ã¿è¾¼ã¿ç”¨ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    if script_dir not in sys.path:
        sys.path.append(script_dir)

    main()
